# II. ACOUSTIC SIMULATIONS


### Página 1

Deep learning-based fil matrices using generativ Christo Department of Si Society for the Advancement of App Berlin, G puhle@ Abstract—In this paper, we present a deep-learning method to filter out effects such as ambient noise, reflections, or source directivity from microphone array data represented as cross- spectral matrices. Specifically, we focus on a generative adver- sarial network (GAN) architecture designed to transform fixed- size cross-spectral matrices. Theses models were trained using sound pressure simulations of varying complexity developed for this purpose. Based on the results from applying these methods in a hyperparameter optimization of an auto-encoding task, we trained the optimized model to perform five distinct transformation tasks derived from different complexities inherent in our sound pressure simulations. Index Terms—Deep learning, cross-spectral matrix, GAN.

## I. INTRODUCTION

As extensively investigated in [5], state-of-the-art deep- learning methods for acoustical sound source localization (SSL) aim to directly reconstruct the direction of arrival of sources or, more generally, the parameters describing the acoustic scene in the presence of reverberation or diffuse noise. This article addresses the problem from a different perspective by employing generative adversarial networks (GANs) to either remove or, at least, reduce the effects of ambient noise, reflections, or source directivity in microphone array data (that is, cross-spectral matrices) before any potential SSL analysis begins. On the one hand, this approach improves the starting point for solving the SSL problem, on the other hand, it enables a more effective use of traditional mapping methods such as standard beamforming or CLEAN-SC. This paper proceeds by first outlining our sound pressure simulation approach in II, then presenting the machine learning model (see III) that we designed to filter simulated cross- spectral matrices. Finally, we discuss results for five different transformation or filtering tasks in IV.

## II. ACOUSTIC SIMULATIONS

A. Basics

### Let ¯p : R3 →C be the complex amplitude of a time-

harmonic sound pressure field of angular frequency ω > 0 This research has been funded by German Federal Ministry for Economic Affairs and Climate Action (Bundesministerium f¨ur Wirtschaft und Kli- maschutz BMWK) under project AntiLerM registration number 49VF220063.

### arXiv:2502.21097v1  [cs.SD]  28 Feb 2025

ltering of cross-spectral ve adversarial networks f Puhle ignal Processing plied Computer Science (GFaI) e.V. Germany @gfai.de and speed of propagation c > 0. By definition, ¯p satisfies the Helmholtz equation ∂2¯p ∂x2 + ∂2¯p ∂y2 + ∂2¯p ∂z2 + k2¯p = 0, k = ω c . (1) Our sign convention for a time-harmonic function is t 7→ exp(iωt). For example, ¯p(x, y, z) = exp  −ik p x2 + y2 + z2  p x2 + y2 + z2 (2) represents an outgoing spherical wave with source in (0, 0, 0).

### Let p : [0, ∞]×[0, π]×[0, 2π] →C be ¯p’s representation in

spherical coordinates (r, θ, ϕ). Solutions to the corresponding Helmholtz equation can be found analytically by assuming

### that p is separable, i.e. there exist functions R : [0, ∞] →C,

### Θ : [0, π] →C, Φ : [0, 2π] →C such that

p(r, θ, ϕ) = R(r) · Θ(θ) · Φ(ϕ). (3) In this case, the Helmholtz equation leads necessarily to R(r) = A · h(1) l (kr) + B · h(2) l (kr) (4) for some constants A, B ∈C, l ∈N0 = {0, 1, . . .}, and h(1) l , h(2) l denote the spherical Hankel functions of the first and second kind of degree l, respectively. Moreover, we have Θ(θ) · Φ(ϕ) = Y m l (θ, ϕ), (5) where m ∈{−l, −l + 1, . . . , l}, and Y m l is the spherical harmonic of degree l and order m. B. Smooth spherical pistons A vibrating spherical cap piston with aperture angle α ∈ (0, π] centered on the north pole of an otherwise rigid sphere with radius r0 > 0 can be described by its surface velocity

### vα : [0, π] × [0, 2π] →R,

vα(θ, ϕ) = V · aα(θ, ϕ),

## V > 0,

(6)

### the corresponding aperture function aα : [0, π] × [0, 2π] →R

is given by aα(θ, ϕ) = 1 −H  θ −α 2  , H(x) = ( 0 x < 0 1 x ≥0 . (7)

### Página 2

The spherical wave spectrum of vα, vα(θ, ϕ) = ∞

## X

l=0 l

## X

m=−l vα lm · Y m l (θ, ϕ), (8) can be computed via integration of the corresponding associ-

### ated Legendre polynomials:

vα lm = V δm0 p (2l + 1)π

## Z 1

cos( α 2 )

## P 0

l (x)dx. (9) Rotating the spherical cap to be centered in the direction (˜θ, ˜ϕ) results in ˜vα lm = r 4π 2l + 1Y m l (˜θ, ˜ϕ)∗· vα lm, (10) respectively. Finally, the radiated pressure in the region r > r0 is completely determined by the surface velocity spectrum (see

### for example [11]):

pvα(r, θ, ϕ) = −iρ0c ∞

## X

l=0 l

## X

m=−l h(2) l (kr) h(2) ′ l (kr0) vα lm · Y m l (θ, ϕ). (11) As most of the higher degrees in (8) are present to form the discontinuity at the boundary of the spherical cap, we opt for a smooth one-parameter family of spherical pistons as fundamental building block of our acoustical models. Its

### surface velocity wα : [0, π] × [0, 2π] →R is defined via

wα(θ, ϕ) =    V · exp  −(1−cos(θ))2 (1−cos( α 2 )) 2  α ≤π, 2π−α π · wπ(θ, ϕ) + α−π π

## · V

π < α ≤2π. (12) Again, we will call α the aperture angle of this piston, but now, when varying θ from 0 to α/2, the particle velocity smoothly changes from V to V/e in the case α ≤π. As before, the spherical wave spectrum of wα can be determined by one- dimensional integration, wα lm = δm0 p (2l + 1)π

## Z 1

−1

## P 0

l (x) ˆwα(x)dx, (13) where ˆwα(cos (θ)) ≡wα(θ, ϕ). Moreover, transformation rule (10) also holds for the coefficients wα lm when rotating the smooth spherical piston to be centered in the direction (˜θ, ˜ϕ), and the radiated sound pressure pwα corresponding to wα can be computed in complete analogy to (11). C. Acoustic models The simulations involved a set of 5000 acoustic models fixed beforehand and each consisting of three (outgoing) smooth spherical pistons which were rotated and translated uniformly at random along the plane z = 0 within a cube of edge length 2.56 m that is centered at the origin. Moreover, each source is furnished with its own reflection plane together with a reflection coefficient between −3 dB and −15 dB. The aperture angles of the pistons vary from 3π/2 to 2π (acoustic monopole) and source radii r0 are chosen randomly and uniformly between 0.1 m and 0.3 m. Within each model, the  source V ’s are chosen to be at most 15 dB below the model sound velocity level, which ranges uniformly between 35 dB and 85 dB across the model set. Consequently, the maximum dynamic range between sources within each model is 15 dB. Model temperatures are taken from a normal distribution with a mean of 20 C◦and a standard deviation of 2.5 C◦. We approximated the sound pressures of these models up to Helmholtz degree 15 at the positions of a virtual microphone array of spherical shape (48 microphones, diameter 0.35 m, centered at (0, 0, d) with d = 2.56 m) for 16 distinct frequen-

### cies:

10 · ∆f, . . . , 25 · ∆f, ∆f = 192000 1024 Hz = 187.5 Hz. (14) In this process, each source of the model set was furnished

### with its own spectral distribution function g : R →[0, 1],

g(f) = exp  −1 2 (f −fc)2 f 2w  , (15) where the center frequency fc was ranging uniformly from 4 · ∆f to 35 · ∆f, and the frequency width parameter fw was chosen between ∆f/2 and 64·∆f, again, uniformly at random. Finally, we included an arbitrary pressure field of degree lmax = 15 (incoming towards the origin) to model ambient

### sound. The upper bound u : {0, . . . , lmax} →R for the

magnitude of the corresponding randomly chosen complex coefficients is given by u(l) = u0 exp  −(lmax + 1) · (l + 1)2 −1 (lmax + 1)2 −1  , (16) where u0 is at least 10 dB below the model sound velocity level.

## III. MACHINE LEARNING MODEL

A. Generative Adversarial Networks The machine learning model we present below is based on a GAN architecture (see [4]), where, when training the model, a pass through the learning loop can be interpreted as a round of a zero-sum game in the sense of game theory. Here, the two players, generator and discriminator, confront each other and aim to optimize their respective objective functions. More precisely, the generator and discriminator are artificial neural networks whose parameters are optimized according to loss functions derived from their respective objective functions.

### In general, the generator G : Z →B is a mapping between

spaces Z and B, where B, on the one hand, contains the set X ⊂B of training data (also referred to as real data) and, on the other hand, defines what is considered to be achievable

### through generation: the elements of the image G(Z) are called

the fake data generated by G from Z. For example, in what follows, B = C48×48×16, therefore, it contains the cross- spectral matrices built from our sound pressure simulations in II-C.

### Now, the discriminator D : B →[0, 1] is a mapping from

B to the unit interval. In each pass through the learning loop, a finite set X ⊆X of training data is drawn randomly (this is also called mini-batching approach [3]) and complemented by

### Página 3

a finite set Z of realizations of a random variable with a fixed probability distribution taking values in Z. The goal of the discriminator is now to distinguish between the real data X and the fake data G(Z) generated from Z. More specifically, D is optimized according to the loss function

## LD = −1

## #X

## X

x∈X log(D(x)) − 1

## #Z

## X

z∈Z log(1 −D(G(z))), (17) where log denotes the natural logarithm. On the opposite side, the objective of the generator is to make the fake data G(Z) generated from Z appear as real as possible from the discriminator’s point of view. This is achieved by optimizing the parameters of G using the loss function

## LG = −1

## #Z

## X

z∈Z log(D(G(z))). (18) In summary, through this adversarial process, both networks are optimized to improve their respective capabilities in pro- cessing the training data (see [8]). B. Complex model building blocks As the ability to take into account phase information will be crucial when working with cross-spectral matrices, all building blocks of the deep neural networks that follow will be real representations of complexifications of their traditional real-

### valued counterparts (see [9]): Suppose L(p) ∈RN×M is a

real matrix representation of a linear network operation, where p ∈RK is the corresponding vector of learnable parameters. Then, simply by switching to the field of complex numbers, we deduce that L(pr) + i L(pi) ∈CN×M for pr, pi ∈RK satisfies (L(pr) + i L(pi))(x + i y) =(L(pr)x −L(pi)y) (19) + i (L(pr)y + L(pi)x) (20) for all x, y ∈RM. Therefore, we will call L(pr) −L(pi) L(pi) L(pr) 

## ∈R2N×2M

(21) a real matrix representation of the complexification of the network operation of L(p). In addition to linear network operations, we make use of two complex phase-preserving activation functions in order to build networks that represent non-linear complex functions. Firstly, we employ a so-called modified ReLU activation function

### FmReLU : C →C with bias b ∈R, which is defined as

### follows (see [1]):

FmReLU(z) = ReLU(|z| + b) z |z| (22) = ( (|z| + b) z |z| if |z| + b ≥0, 0 otherwise. (23)

### And secondly, we apply a leaky variant FlCard : C →C,

FlCard(z) = 1 2 ((1 + α) + cos(arg(z))) z, α > 0, (24)  of the complex cardioid activation function of [10]. The absolute value of the latter depends on the phase of z, whereas the former does not. C. Models for generator and discriminator The generator G = GE ◦GD we are going to employ is a composition of two convolutional neural networks, encoder GE and decoder GD. The input of the encoder GE takes values of C48×48×16, hence, it can be applied to cross-spectral matrices built out of the simulations of II-C. The kernel of the utilized (transposed) two-dimensional complex bias-free convolutions is 48×48 with trivial stride. Therefore, a padding strategy was not necessary. The encoder GE is composed of one such convolutional layer c and one to four complex bias- free dense layers d (cf. III-B),   48 48 16  → c,a   1 1 ngen  → d,a   1 1 nden  → d,a . . . → d,a   1 1 nden  . (25) Here, every convolution or dense layer entails an activation a using one of the functions of III-B,    d1 ... dk    (26) is an alternative notation for Cd1×...×dk, the number of con- volution filters is ngen ∈{32, 64}, and the number of dense units is nden ∈{512, 1024}. The decoder GD realizes   1 1 nden  → d,a . . . → d,a   1 1 nden  → d,a   1 1 ngen  → ct,a   48 48 16   (27) (ct denotes transposed convolution) followed by a Hermitian-

### izing operation H : C48×48×16 →C48×48×16,

(H(C))ijk = 1 2  Cijk + C∗ jik  . (28) The discriminator D is similar in structure to the encoder, but with removed dense layers, and the number of convolution

### filters now is ndis ∈{16, 32}:

  48 48 16  → c,a   1 1 ndis  → sig [0, 1]. (29) Moreover, a real sigmoid activation function sig is applied the real and the imaginary parts of the convolution output (a so- called split-type A activation, see [2]), and the corresponding result is averaged over all dimensions of its real representation. D. Training set and loop Following the notation of III-A, we now fix

## Z = C48×48×16,

## B = C48×48×16.

(30) Moreover, we decide on a transformation rule the generator G is intended to learn (cf. IV-B). More precisely, we decide

### on sets X ⊂B and Y ⊂B and a map f : X →Y. For

### Página 4

example, in the most trivial case, G could be trained to work as an auto-encoder on a fixed set X ⊂B by setting Y = X and f = Id. In each pass through the training loop, zxi ∈ZX and zyi ∈ ZY is generated out of xi ∈X and yi = f(xi) ∈Y = f(X) (with X being a mini-batch drawn randomly from X, and Z = ZX, see III-A) by adding noise, respectively. Based on III-A, we add another term to LG to integrate the transformation rule intended for G,

## LG = −1

## N

## N

## X

i=1 log(D(G(zxi))) (31) + λ

## 2N

## N

## X

i=1 ε(yi, G(zxi)) + ε(yi, G(zyi)). (32) Here, N = #X, λ > 0, ε(a, b) = 1

## K

## K

## X

k=1 d(πk(a), πk(b)), (33) d(ma, mb) =κ  1 −tr(ma · mb) ∥ma∥∥mb∥  (34) + (1 −κ) |∥ma∥−∥mb∥| , (35) K = 16, κ = 9/10. Moreover, ∥· ∥denotes the Frobenius

### norm and πk : C48×48×16 →C48×48 is the projection onto

the k-th component, πk(C) = (Cijk)i,j=1,...,48. (36) To summarize (31) and (32), G is sought to be a denoising opponent to the discriminator that, on the one hand, realizes the map f on elements of X, and, on the other hand, is an auto-encoder for the elements of Y.

## IV. RESULTS

A. Hyperparameter optimization Some of the parameters involved in the training process were chosen based on preliminary experiments or computa- tional limitations. For example, we fixed the mini-batch size to be N = 16, and the size of the training data set was chosen as #X = 2560. Moreover, each component πk(C) of C ∈X, Y was normalized with respect to its Frobenius norm as a preprocessing step after the pressure simulations of II-C. As a consequence, we were able to balance (31) and (32) in terms of their magnitude by setting λ = 200. For the stochastic gradient descent of generator and discriminator, we used Adam optimizers with β1 = 0.5, β2 = 0.999, ϵ = 10−7 without exponential moving average (see [7]). The remaining parameters were chosen in a hyperparameter optimization. As metric to assess the quality of this optimiza- tion, we opted for the average accuracy gacc(G) = 1 − 1 #Xtest

## X

x∈Xtest ε(f(x), G(x)) (37) on a test data set Xtest of 512 elements disjoint to X using our weighted distance function ε (see (33)) that utilizes  the correlation matrix distance of [6]. The 512 parameter combinations that were tested included the number of con- volution filters in the generator ngen ∈{32, 64} and the discriminator ndis ∈{16, 32}, the number of dense units nden ∈{512, 1024} and dense layers nlay ∈{1, 2, 3, 4} in the encoder and decoder, the learning rates lr ∈{2·10−4, 2·10−5} of the Adam optimizers for generator and discriminator, and

### the used activation function: FmReLU with b ∈{−1/8, −1/4}

or FlCard with α ∈{0, 1/2}. The combination ngen = 64, ndis = 16, nden = 512, (38) nlay = 1, lgen r = ldis r = 2 · 10−5 (39) together with FlCard, α = 1/2 yielded the maximum value of gacc(G) = 0.9866 after 100 epochs of training using Y = X and f = Id (more specifically, task 1) in IV-B). B. Transformation tasks After hyperparameter optimization, we investigated 5 trans- formation tasks G was intended to learn. For each of these tasks, we started by selecting a set M = {m1, . . . , mM} of M = 2560 models from our model set we presented in II-C. We then simulated each of these models mi in two different complexities creating xi ∈C48×48×16 and yi ∈C48×48×16. The latter built up X and Y, respectively,

### and we set f(xi) = yi. We considered the following pairings:

1) Auto-encoder pairing, xi = yi 2) xi exhibits ambient sound, yi does not 3) xi exhibits reflections, yi does not 4) xi exhibits directivity, yi does not 5) xi exhibits directivity, reflections and ambient sound, yi none of these If not stated otherwise here, a model was simulated with monopole sources, without reflections and with no ambient sound present. After 1000 epochs of training, we tested the resulting generator G by reiterating the previous steps for a model set Mtest = {m1, . . . , mMtest} disjoint to M, Mtest = 512. For each element x of the corresponding set Xtest, we evaluated gx acc(G) = 1 −ε(f(x), G(x)), (40) and gacc(G) (see (37)) is simply the average of these results, gacc(G) = 1 #Xtest

## X

x∈Xtest gx acc(G). (41) For example, the results of the auto-encoder transformation task 1) can be found in fig. 1. The average accuracy was gacc(G) = 0.9948 in this case serving as baseline, as the corresponding results of all other cases are expected to be lower or equal. In addition, we place these values side-by- side with gx acc(Id) = 1 −ε(f(x), x) and its average gacc(Id), respectively, in order to quantify the initial situation in com- parison for all transformation tasks (see figs. 1-5). The lowest resulting gacc(G) values were achieved in cases 4) and 5) where the corresponding transformation task in- cluded to remove the directivity information from the data.

### Página 5

Fig. 4. Accuracy scatter plot for transformation task 4) (directivity) Fig. 5. Accuracy scatter plot for transformation task 5) (directivity, reflections and ambient sound)

## REFERENCES

[1] M. Arjovsky, A. Shah, and Y. Bengio, “Unitary evolution recurrent neural networks,” Proceedings of the 33rd International Conference on Machine Learning (ICML’16), 2016. [2] J. Bassey, “A Survey of Complex-Valued Neural Networks,” 2021,

### arXiv:2101.12249. [Online]. Available: https://arxiv.org/abs/2101.12249.

### [3] L. Bottou: “Online Algorithms and Stochastic Approximations,” Cam-

bridge University Press, 1998. [4] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative Adversarial Nets,” Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS 2014), 2014. [5] P.-A. Grumiaux, S. Kiti´c, L. Girin, and A. Gu´erin, “A survey of sound source localization with deep learning methods,” J. Acoust. Soc. Am. 152, 107–151, 2022. [6] M. Herdin, N. Czink, H. Ozcelik, and E. Bonek, “Correlation matrix distance, a meaningful measure for evaluation of non-stationary MIMO channels ,” Proceedings of the 2005 IEEE 61st Vehicular Technology Conference (VETECS 2005), 2005.

### [7] D. Kingma, and J. L. Ba, “Adam: A Method for Stochastic Optimiza-

tion,” Proceedings of the 3rd International Conference for Learning Representations (ICLR 2015), 2015. [8] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, “Improved techniques for training GANs,” Advances in Neural Information Processing Systems 29 (NIPS 2016), 2016. [9] C. Trabelsi, O. Bilaniuk, Y. Zhang, D. Serdyuk, S. Subramanian, J. F. Santos, S. Mehri, N. Rostamzadeh, Y. Bengio, and C. J. Pal, “Deep Complex Networks,” Proceedings of the 6th International Conference on Learning Representations (ICLR 2018), 2018.

### [10] P. Virtue, X. Y. Stella, and M. Lustig, “Better than real: complex-

valued neural nets for MRI fingerprinting,” Proceedings of the 2017 IEEE International Conference on Image Processing (ICIP 2017), 2017.

### [11] E. G. Williams, “Fourier acoustics: sound radiation and nearfield acous-

tical holography,” Academic Press, 1999.  This could be expected as the microphone array used in the simulations was only of small aperture compared to size of the total acoustic scene. Moreover, when analyzing gacc(G) as a function of learning epoch, it became clear, that the training progress was much slower and did not converge within 1000 epochs of training in the cases 4) and 5). Studying fig. 3 more carefully, we realize that there are many models mi ∈Mtest with ε(f(xi), xi) = 0 for transformation task 3). This is due to the fact that although a reflection plane was present, its position did not render a reflection possible. Fig. 1. Accuracy scatter plot for transformation task 1) (auto-encoder) Fig. 2. Accuracy scatter plot for transformation task 2) (ambient sound) Fig. 3. Accuracy scatter plot for transformation task 3) (reflections)